<div align="center">
  <img src="assets/loco_operator.png" width="55%" alt="LocoOperator" />
</div>

<br>

<div align="center">

[![MODEL](https://img.shields.io/badge/Model-FFB300?style=for-the-badge&logo=huggingface&logoColor=white)](https://huggingface.co/FutureMa)
[![DATA](https://img.shields.io/badge/Data-0040A1?style=for-the-badge&logo=huggingface&logoColor=ffffff)](https://huggingface.co/datasets/FutureMa/cc)
[![BASE](https://img.shields.io/badge/Base_Model-7C3AED?style=for-the-badge&logo=huggingface&logoColor=white)](https://huggingface.co/Qwen/Qwen3-4B-Instruct-2507)

</div>

> **LocoOperator-4B** is a 4B-parameter code exploration agent distilled from Qwen3-Coder-Next. Designed as a local sub agent for Claude Code-style agent loops â€” fast codebase navigation at zero API cost.

## ğŸ“‹ Table of Contents

- ğŸ“° [News & Updates](#-news--updates)
- ğŸ“ [Introduction](#-introduction)
- âœ¨ [Key Features](#-key-features)
- ğŸ—ï¸ [Architecture](#ï¸-architecture)
- ğŸ“ˆ [Performance](#-performance)
- ğŸš€ [Quick Start](#-quick-start)
- âš ï¸ [Known Limitations](#ï¸-known-limitations)
- ğŸ“„ [License](#-license)
- ğŸ™ [Acknowledgments](#-acknowledgments)

## ğŸ“° News & Updates

- **\[2026-02-22\]** ğŸ‰ LocoOperator-4B model card and evaluation analysis released.
- **\[2026-02-20\]** ğŸš€ LocoOperator-4B (Step 2524) training completed. GGUF quantized for local deployment.

## ğŸ“ Introduction

LocoOperator-4B is a tool-calling agent model trained via knowledge distillation from **Qwen3-Coder-Next** inference traces. It specializes in multi-turn codebase exploration â€” reading files, searching code, and navigating project structures within a Claude Code-style agent loop.

<div align="center">

|  | LocoOperator-4B |
|:--|:--|
| **Base Model** | [Qwen3-4B-Instruct-2507](https://huggingface.co/Qwen/Qwen3-4B-Instruct-2507) |
| **Teacher Model** | Qwen3-Coder-Next |
| **Training Method** | Full-parameter SFT (distillation) |
| **Training Data** | 170,356 multi-turn conversation samples |
| **Max Sequence Length** | 16,384 tokens |
| **Training Hardware** | 4x NVIDIA H200 141GB SXM5 |
| **Training Time** | ~25 hours |
| **Framework** | MS-SWIFT |

</div>

## âœ¨ Key Features

- ğŸ”§ **Tool-Calling Agent**: Generates structured `<tool_call>` JSON for Read, Grep, Glob, Bash, Write, Edit, and Task (subagent delegation)
- ğŸ¯ **100% JSON Validity**: Every tool call is valid JSON with all required arguments â€” outperforming the teacher model (87.6%)
- ğŸ  **Local Deployment**: GGUF quantized, runs on Mac Studio via llama.cpp at zero API cost
- âš¡ **Lightweight Explorer**: 4B parameters, optimized for fast codebase search and navigation
- ğŸ”„ **Multi-Turn**: Handles conversation depths of 3â€“33 messages with consistent tool-calling behavior

## ğŸ—ï¸ Architecture

LocoOperator-4B operates as a **sub agent (explorer)** within a two-tier agent system:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Main Agent                      â”‚
â”‚            Qwen3-Coder-Next                      â”‚
â”‚   (planning, code generation, editing)           â”‚
â”‚                                                  â”‚
â”‚   Delegates exploration tasks â”€â”€â”€â”€â”€â”€â”            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Explorer Sub Agent                  â”‚
â”‚            LocoOperator-4B                       â”‚
â”‚   (Glob, Grep, Read, lightweight Bash)           â”‚
â”‚                                                  â”‚
â”‚   GGUF Â· llama.cpp Â· Mac Studio                  â”‚
â”‚   50K context Â· 10 turns per task                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

The main agent handles decision-making and code generation while delegating codebase exploration to LocoOperator-4B â€” keeping API costs low and latency minimal.

## ğŸ“ˆ Performance

Evaluated on 65 multi-turn conversation samples from diverse open-source projects (scipy, fastapi, arrow, attrs, gevent, gunicorn, etc.), with labels generated by Qwen3-Coder-Next.

### Core Metrics

<div align="center">

| Metric | Score |
|:-------|:-----:|
| **Tool Call Presence Alignment** | **100%** (65/65) |
| **First Tool Type Match** | **65.6%** (40/61) |
| **JSON Validity** | **100%** (76/76) |
| **Argument Syntax Correctness** | **100%** (76/76) |

</div>

The model perfectly learned *when* to use tools vs. when to respond with text (100% presence alignment). Tool type mismatches are between semantically similar tools (e.g. Grep vs Read) â€” different but often valid strategies.

### Tool Distribution Comparison

<div align="center">

| Tool | LocoOperator-4B | Qwen3-Coder-Next | Delta |
|:-----|:---------------:|:-----------------:|:-----:|
| Read | 22 | 32 | -10 |
| Bash | 22 | 17 | +5 |
| Grep | 14 | 18 | -4 |
| Glob | 9 | 11 | -2 |
| Task | 7 | 7 | 0 |
| Write | 2 | 3 | -1 |
| **Total** | **76** | **89** | **-13** |

</div>

### JSON & Argument Syntax Correctness

<div align="center">

| Model | JSON Valid | Argument Syntax Valid |
|:------|:---------:|:--------------------:|
| **LocoOperator-4B** | 76/76 (100%) | 76/76 (100%) |
| Qwen3-Coder-Next (teacher) | 89/89 (100%) | 78/89 (87.6%) |

</div>

> LocoOperator-4B achieves perfect structured output. The teacher model has 11 tool calls with missing required arguments (empty `arguments: {}`).

## ğŸš€ Quick Start

### Prerequisites

- ğŸ–¥ï¸ **Mac Studio** (or any machine with sufficient RAM for 4B GGUF)
- ğŸ“¦ **llama.cpp** ([Installation guide](https://github.com/ggerganov/llama.cpp))

### Serve with llama.cpp

```bash
# Download the GGUF model
# (replace with actual model path)

# Start the server
./llama-server \
    -m LocoOperator-4B.gguf \
    --ctx-size 51200 \
    --host 0.0.0.0 \
    --port 8080
```

### Recommended Settings

| Parameter | Value | Rationale |
|:----------|:------|:----------|
| Context size | 50K | Covers multi-turn exploration with room for tool outputs |
| Max turns | 10 | Sufficient for focused codebase exploration tasks |
| Temperature | 0.7 | Balanced between determinism and exploration |

## Training Details

<details>
  <summary>ğŸ“‹ Click to expand full training configuration</summary>

| Parameter | Value |
|:----------|:------|
| Base model | Qwen3-4B-Instruct-2507 |
| Teacher model | Qwen3-Coder-Next |
| Method | Full-parameter SFT |
| Training data | 170,356 samples |
| Hardware | 4x NVIDIA H200 141GB SXM5 |
| Parallelism | DDP (no DeepSpeed) |
| Precision | BF16 |
| Epochs | 1 |
| Batch size | 2/GPU, gradient accumulation 4 (effective batch 32) |
| Learning rate | 2e-5, warmup ratio 0.03 |
| Max sequence length | 16,384 tokens |
| Template | qwen3_nothinking |
| Framework | MS-SWIFT |
| Training time | ~25 hours |
| Checkpoint | Step 2524 |

</details>

## âš ï¸ Known Limitations

- First-tool-type match is 65.6% â€” the model sometimes picks a different (but not necessarily wrong) tool than the teacher
- Tends to under-generate parallel tool calls compared to the teacher (76 vs 89 total calls across 65 samples)
- Preference for Bash over Read may indicate the model defaults to shell commands where file reads would be more appropriate
- Evaluated on 65 samples only; larger-scale evaluation needed

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- ğŸ¤– **[Qwen Team](https://huggingface.co/Qwen)** for the Qwen3-4B-Instruct-2507 base model
- ğŸ› ï¸ **[MS-SWIFT](https://github.com/modelscope/ms-swift)** for the training framework
- ğŸ¦™ **[llama.cpp](https://github.com/ggerganov/llama.cpp)** for efficient local inference
- ğŸ¤– **[Anthropic](https://www.anthropic.com/)** for the Claude Code agent loop design that inspired this work

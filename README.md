<div align="center">
  <img src="assets/loco_operator.png" width="55%" alt="LocoOperator" />
</div>

<br>

<div align="center">

[![MODEL](https://img.shields.io/badge/Model-FFB300?style=for-the-badge&logo=huggingface&logoColor=white)](https://huggingface.co/LocoreMind/LocoOperator-4B)
[![Blog](https://img.shields.io/badge/Blog-4285F4?style=for-the-badge&logo=google-chrome&logoColor=white)](https://locoremind.com/blog/loco-operator)
[![GitHub](https://img.shields.io/badge/GitHub-181717?style=for-the-badge&logo=github&logoColor=white)](https://github.com/LocoreMind/LocoOperator)
[![Colab](https://img.shields.io/badge/Colab-F9AB00?style=for-the-badge&logo=googlecolab&logoColor=white)](https://colab.research.google.com/github/LocoreMind/LocoOperator/blob/main/LocoOperator_4B.ipynb)

</div>

> **LocoOperator-4B** is a 4B-parameter code exploration agent distilled from Qwen3-Coder-Next. Designed as a local sub agent for Claude Code-style agent loops â€” fast codebase navigation at zero API cost.

## ğŸ“‹ Table of Contents

- ğŸ“° [News & Updates](#-news--updates)
- ğŸ“ [Introduction](#-introduction)
- âœ¨ [Key Features](#-key-features)
- ğŸ—ï¸ [Architecture](#ï¸-architecture)
- ğŸ“ˆ [Performance](#-performance)
- ğŸš€ [Quick Start](#-quick-start)
- ğŸ”§ [Analysis Pipeline](#-analysis-pipeline)
- ğŸ“ [Project Structure](#-project-structure)
- âš ï¸ [Known Limitations](#ï¸-known-limitations)
- ğŸ“„ [License](#-license)
- ğŸ™ [Acknowledgments](#-acknowledgments)

## ğŸ“° News & Updates

- **\[2026-02-23\]** ğŸ‰ LocoOperator-4B model card and evaluation analysis released.
- **\[2026-02-20\]** ğŸš€ LocoOperator-4B (Step 2524) training completed. GGUF quantized for local deployment.

## ğŸ“ Introduction

LocoOperator-4B is a tool-calling agent model trained via knowledge distillation from **Qwen3-Coder-Next** inference traces. It specializes in multi-turn codebase exploration â€” reading files, searching code, and navigating project structures within a Claude Code-style agent loop.

<div align="center">

|  | LocoOperator-4B |
|:--|:--|
| **Base Model** | [Qwen3-4B-Instruct-2507](https://huggingface.co/Qwen/Qwen3-4B-Instruct-2507) |
| **Teacher Model** | Qwen3-Coder-Next |
| **Training Method** | Full-parameter SFT (distillation) |
| **Training Data** | 170,356 multi-turn conversation samples |
| **Max Sequence Length** | 16,384 tokens |
| **Training Hardware** | 4x NVIDIA H200 141GB SXM5 |
| **Training Time** | ~25 hours |
| **Framework** | MS-SWIFT |

</div>

## âœ¨ Key Features

- ğŸ”§ **Tool-Calling Agent**: Generates structured `<tool_call>` JSON for Read, Grep, Glob, Bash, Write, Edit, and Task (subagent delegation)
- ğŸ¯ **100% JSON Validity**: Every tool call is valid JSON with all required arguments â€” outperforming the teacher model (87.6%)
- ğŸ  **Local Deployment**: GGUF quantized, runs on Mac Studio via llama.cpp at zero API cost
- âš¡ **Lightweight Explorer**: 4B parameters, optimized for fast codebase search and navigation
- ğŸ”„ **Multi-Turn**: Handles conversation depths of 3â€“33 messages with consistent tool-calling behavior

## ğŸ—ï¸ Architecture

LocoOperator-4B operates as a **sub agent (explorer)** within a two-tier agent system:

<div align="center">
  <img src="assets/architecture.png" width="80%" alt="Architecture" />
</div>

The main agent handles decision-making and code generation while delegating codebase exploration to LocoOperator-4B â€” keeping API costs low and latency minimal.

## ğŸ“ˆ Performance

Evaluated on 65 multi-turn conversation samples from diverse open-source projects (scipy, fastapi, arrow, attrs, gevent, gunicorn, etc.), with labels generated by Qwen3-Coder-Next.

### Core Metrics

<div align="center">

| Metric | Score |
|:-------|:-----:|
| **Tool Call Presence Alignment** | **100%** (65/65) |
| **First Tool Type Match** | **65.6%** (40/61) |
| **JSON Validity** | **100%** (76/76) |
| **Argument Syntax Correctness** | **100%** (76/76) |

</div>

The model perfectly learned *when* to use tools vs. when to respond with text (100% presence alignment). Tool type mismatches are between semantically similar tools (e.g. Grep vs Read) â€” different but often valid strategies.

### Tool Distribution Comparison

<div align="center">

| Tool | LocoOperator-4B | Qwen3-Coder-Next | Delta |
|:-----|:---------------:|:-----------------:|:-----:|
| Read | 22 | 32 | -10 |
| Bash | 22 | 17 | +5 |
| Grep | 14 | 18 | -4 |
| Glob | 9 | 11 | -2 |
| Task | 7 | 7 | 0 |
| Write | 2 | 3 | -1 |
| **Total** | **76** | **89** | **-13** |

</div>

### JSON & Argument Syntax Correctness

<div align="center">

| Model | JSON Valid | Argument Syntax Valid |
|:------|:---------:|:--------------------:|
| **LocoOperator-4B** | 76/76 (100%) | 76/76 (100%) |
| Qwen3-Coder-Next (teacher) | 89/89 (100%) | 78/89 (87.6%) |

</div>

> LocoOperator-4B achieves perfect structured output. The teacher model has 11 tool calls with missing required arguments (empty `arguments: {}`).

## ğŸš€ Quick Start

### Prerequisites

- **Claude Code** â€” `npm install -g @anthropic-ai/claude-code`
- **llama.cpp** â€” build from source or `brew install llama.cpp`
- **uv** â€” `curl -LsSf https://astral.sh/uv/install.sh | sh`
- **OpenRouter API key** â€” https://openrouter.ai/keys

### Serve with llama.cpp

```bash
# Download the GGUF model
# (replace with actual model path)

# Start the server
./llama-server \
    -m LocoOperator-4B.gguf \
    --ctx-size 51200 \
    --host 0.0.0.0 \
    --port 8080
```

### Recommended Settings

| Parameter | Value | Rationale |
|:----------|:------|:----------|
| Context size | 50K | Covers multi-turn exploration with room for tool outputs |
| Max turns | 10 | Sufficient for focused codebase exploration tasks |
| Temperature | 0.7 | Balanced between determinism and exploration |

## ğŸ”§ Analysis Pipeline

This repo includes a hybrid analysis pipeline that combines LocoOperator-4B (local) with a cloud LLM via OpenRouter, orchestrated through `claude -p`.

```
claude -p (sonnet) â”€â”€â†’ proxy (9091) â”€â”€â†’ OpenRouter Qwen3-Coder-Next
  â””â”€ subagent (haiku) â”€â†’ proxy (9091) â”€â”€â†’ local llama-server (8080)
```

The main agent runs as sonnet (cloud), and when it spawns subagents (Task tool) they default to haiku, which the proxy routes to the local 4B model. If the local model hits context limits or exceeds 10 turns, the proxy automatically falls back to OpenRouter.

The proxy (`scripts/proxy.py`) handles:
- Anthropic Messages API â†” OpenAI Chat Completions format conversion for the local model
- Parsing `<tool_call>` text output from the local model back into Anthropic tool_use blocks
- Automatic fallback to OpenRouter on context overflow

### Setup

```bash
# Install Python dependencies
uv sync

# Configure your OpenRouter API key
cp .env.example .env
# Edit .env and set OPENROUTER_API_KEY
```

`.claude/settings.local.json` is auto-generated on first run from your `.env` key. No need to create it manually.

Place your GGUF model at `models/LocoOperator-4B-GGUF/LocoOperator-4B.gguf`.

Place target projects under `data/repos/`.

### Single query test

```bash
./scripts/test_single.sh tqdm "How does tqdm detect if running in a Jupyter notebook?"
```

### Batch analyze

```bash
./scripts/analyze.sh tqdm
```

This reads queries from `data/queries/tqdm-queries.txt` and saves results to `data/outputs/tqdm/`.

### Adding More Projects

1. Clone a project into `data/repos/`:
   ```bash
   git clone --depth 1 https://github.com/user/repo data/repos/repo
   rm -rf data/repos/repo/.git
   ```
2. Create a queries file at `data/queries/repo-queries.txt` (tab-separated `id\tquery`)
3. Run: `./scripts/analyze.sh repo`

## ğŸ“ Project Structure

```
LocoOperator/
â”œâ”€â”€ .env.example                         # OpenRouter key template
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ models/LocoOperator-4B-GGUF/
â”‚   â””â”€â”€ LocoOperator-4B.gguf
â”œâ”€â”€ examples/                            # model inference examples
â”‚   â”œâ”€â”€ quick_start.py
â”‚   â””â”€â”€ codebase_analysis_example.py
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ repos/                           # target projects to analyze
â”‚   â”œâ”€â”€ queries/tqdm-queries.txt         # analysis queries (tab-separated: id\tquery)
â”‚   â””â”€â”€ outputs/                         # analysis results
â”œâ”€â”€ prompts/
â”‚   â””â”€â”€ analyze_query.txt                # prompt template
â””â”€â”€ scripts/
    â”œâ”€â”€ proxy.py                         # hybrid routing proxy
    â”œâ”€â”€ setup.sh                         # auto-generates .claude/settings.local.json
    â”œâ”€â”€ start_services.sh                # auto-starts llama-server + proxy
    â”œâ”€â”€ analyze.sh                       # batch analysis runner
    â””â”€â”€ test_single.sh                   # single query test
```

## Training Details

<details>
  <summary>ğŸ“‹ Click to expand full training configuration</summary>

| Parameter | Value |
|:----------|:------|
| Base model | Qwen3-4B-Instruct-2507 |
| Teacher model | Qwen3-Coder-Next |
| Method | Full-parameter SFT |
| Training data | 170,356 samples |
| Hardware | 4x NVIDIA H200 141GB SXM5 |
| Parallelism | DDP (no DeepSpeed) |
| Precision | BF16 |
| Epochs | 1 |
| Batch size | 2/GPU, gradient accumulation 4 (effective batch 32) |
| Learning rate | 2e-5, warmup ratio 0.03 |
| Max sequence length | 16,384 tokens |
| Template | qwen3_nothinking |
| Framework | MS-SWIFT |
| Training time | ~25 hours |
| Checkpoint | Step 2524 |

</details>

## âš ï¸ Known Limitations

- First-tool-type match is 65.6% â€” the model sometimes picks a different (but not necessarily wrong) tool than the teacher
- Tends to under-generate parallel tool calls compared to the teacher (76 vs 89 total calls across 65 samples)
- Preference for Bash over Read may indicate the model defaults to shell commands where file reads would be more appropriate
- Evaluated on 65 samples only; larger-scale evaluation needed

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- ğŸ¤– **[Qwen Team](https://huggingface.co/Qwen)** for the Qwen3-4B-Instruct-2507 base model
- ğŸ› ï¸ **[MS-SWIFT](https://github.com/modelscope/ms-swift)** for the training framework
- ğŸ¦™ **[llama.cpp](https://github.com/ggerganov/llama.cpp)** for efficient local inference
- ğŸ¤– **[Anthropic](https://www.anthropic.com/)** for the Claude Code agent loop design that inspired this work
